# -*- coding: utf-8 -*-
import logging
import os
import json
import time
import random
from datetime import datetime, timezone
from typing import Dict, List, Tuple, Any, Optional

import asyncio
import httpx
from difflib import SequenceMatcher
import re

# –ò–º–ø–æ—Ä—Ç curl-cffi –¥–ª—è –æ–±—Ö–æ–¥–∞ TLS fingerprinting
try:
    from curl_cffi.requests import AsyncSession
    CURL_CFFI_AVAILABLE = True
except ImportError:
    CURL_CFFI_AVAILABLE = False
    AsyncSession = None

from .storage import load_cache, save_cache, compute_hash, get_cache_stats
from .config import (
    PROJECT_ROOT, SOURCES, USE_PROXY, PROXY_URL, PROXY_URL_EU,
    PROXY_PROVIDER, PROXY_STICKY, PROXY_FALLBACK_EU,
    SOCKS5_URL, SOCKS5_URL_EU, HTTP_TUNNEL_URL
)
from .html_clean import clean_html
from .summarize import summarize_rules, normalize_plain, extract_sections

log = logging.getLogger(__name__)

TRANS_CACHE_FILE = PROJECT_ROOT / "data" / "trans_cache.json"
if TRANS_CACHE_FILE.exists():
    try:
        trans_cache: Dict[str, str] = json.loads(TRANS_CACHE_FILE.read_text(encoding="utf-8"))
    except Exception:
        trans_cache = {}
else:
    trans_cache = {}

TIMEOUT = httpx.Timeout(30.0, connect=15.0)  # –£–≤–µ–ª–∏—á–∏–ª–∏ timeout

# –Ø–∑—ã–∫–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º (–¥–ª—è Accept-Language)
_DEFAULT_LANG_BY_REGION = {
    "EU": "en-GB,en;q=0.9",
    "MD": "en-GB,en;q=0.9,ro;q=0.8,ru;q=0.7",
    "GLOBAL": "en-US,en;q=0.9",
}

def _get_proxy_for_region(region: str, proxy_country: Optional[str] = None, session_id: Optional[str] = None) -> Optional[Dict[str, str]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–∫—Å–∏ –¥–ª—è httpx –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–≥–∏–æ–Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞.
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - region: EU, MD –∏–ª–∏ GLOBAL
    - proxy_country: –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω—ã –¥–ª—è –ø—Ä–æ–∫—Å–∏ (–∏–∑ config.json –∏—Å—Ç–æ—á–Ω–∏–∫–∞)
    - session_id: –¥–ª—è sticky-—Å–µ—Å—Å–∏–π (Froxy –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç session=<rand>)
    """
    log.debug(f"üîç DEBUG: _get_proxy_for_region({region}, USE_PROXY={USE_PROXY}, PROXY_URL={bool(PROXY_URL)}, PROXY_URL_EU={bool(PROXY_URL_EU)})")
    
    if not USE_PROXY:
        log.debug(f"üö´ USE_PROXY=False, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None")
        return None
    
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç SOCKS5 –Ω–∞–¥ HTTP (–¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫ Railway)
    if region == "MD" and SOCKS5_URL:
        base_url = SOCKS5_URL
        is_socks = True
    elif region == "EU" and SOCKS5_URL_EU:
        base_url = SOCKS5_URL_EU
        is_socks = True
    elif SOCKS5_URL:
        base_url = SOCKS5_URL
        is_socks = True
    elif region == "MD" and PROXY_URL:
        base_url = PROXY_URL
        is_socks = False
    elif region == "EU" and PROXY_URL_EU:
        base_url = PROXY_URL_EU
        is_socks = False
    elif PROXY_URL:
        base_url = PROXY_URL
        is_socks = False
    else:
        return None
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ Froxy –ø—Ä–æ–∫—Å–∏
    if PROXY_PROVIDER == "froxy":
        if PROXY_STICKY and session_id:
            # –î–æ–±–∞–≤–ª—è–µ–º session –¥–ª—è sticky —Å–µ—Å—Å–∏–π
            modified_url = base_url.replace("@proxy.froxy.com", f":session={session_id}@proxy.froxy.com")
            log.debug(f"üîê Froxy sticky session: region={region}, session={session_id}")
            return {"http://": modified_url, "https://": modified_url}
        else:
            log.debug(f"üîê Froxy –ø—Ä–æ–∫—Å–∏: region={region}")
            return {"http://": base_url, "https://": base_url}
    else:
        # –î—Ä—É–≥–∏–µ –ø—Ä–æ–∫—Å–∏-–ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã
        log.debug(f"üîê –ü—Ä–æ–∫—Å–∏: region={region}, provider={PROXY_PROVIDER}")
        return {"http://": base_url, "https://": base_url}


# –†–æ—Ç–∞—Ü–∏—è User-Agent –¥–ª—è –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:132.0) Gecko/20100101 Firefox/132.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.6 Safari/605.1.15",
]

def _fix_facebook_url(url: str) -> str:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä _fb_noscript=1 –∫ Facebook/Meta URL –¥–ª—è –æ–±—Ö–æ–¥–∞ JavaScript —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤
    """
    if any(domain in url for domain in ["facebook.com", "transparency.meta.com", "about.fb.com", "developers.facebook.com"]):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä —É–∂–µ –µ—Å—Ç—å
        if "_fb_noscript=1" not in url:
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä
            separator = "&" if "?" in url else "?"
            url = f"{url}{separator}_fb_noscript=1"
            log.debug(f"üìã –î–æ–±–∞–≤–ª–µ–Ω _fb_noscript –∫: {url}")
    return url

async def _fetch_with_curl_cffi(url: str, headers: dict, proxies: Optional[dict] = None, timeout: float = 30.0):
    """
    –ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è curl-cffi, –∫–æ—Ç–æ—Ä—ã–π –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç —Å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º httpx.Response
    """
    if not CURL_CFFI_AVAILABLE:
        raise ImportError("curl-cffi not available")
    
    class CurlCffiResponse:
        def __init__(self, response):
            self._response = response
            self.status_code = response.status_code
            self.text = response.text
            self.content = response.content
        
        def raise_for_status(self):
            if 400 <= self.status_code < 600:
                raise httpx.HTTPStatusError(f"{self.status_code} Error", request=None, response=self)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º proxies –∏–∑ httpx —Ñ–æ—Ä–º–∞—Ç–∞ –≤ curl-cffi
    curl_proxies = None
    if proxies:
        # httpx: {"http://": "...", "https://": "..."} -> curl-cffi: {"http": "...", "https": "..."}
        proxy_url = proxies.get("http://", proxies.get("https://"))
        
        # –ü—Ä–æ–∫—Å–∏ —É–∂–µ –≤ HTTP —Ñ–æ—Ä–º–∞—Ç–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å
        
        curl_proxies = {
            "http": proxy_url,
            "https": proxy_url
        }
    
    async with AsyncSession(
        impersonate="chrome120",  # TLS fingerprint Chrome 120
        verify=False,  # –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É SSL
        timeout=timeout
    ) as session:
        response = await session.get(
            url,
            headers=headers,
            proxies=curl_proxies,
            allow_redirects=True
        )
        return CurlCffiResponse(response)

def _get_random_headers(url: str = "", accept_lang: Optional[str] = None):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
    ua = random.choice(USER_AGENTS)
    headers = {
        "User-Agent": ua,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": accept_lang or "en-US,en;q=0.9",
        "Accept-Encoding": "gzip, deflate, br",
        "Cache-Control": "max-age=0",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Sec-Fetch-User": "?1",
        "Sec-CH-UA": '"Not_A Brand";v="8", "Chromium";v="131"',
        "Sec-CH-UA-Mobile": "?0",
        "Sec-CH-UA-Platform": '"Windows"',
    }
    
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è Meta/Facebook —Å–∞–π—Ç–æ–≤
    if any(domain in url for domain in ["facebook.com", "transparency.meta.com", "about.fb.com", "developers.facebook.com"]):
        headers["Referer"] = "https://www.google.com/"
        headers["Sec-Fetch-Site"] = "cross-site"
        headers["Accept"] = "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
        # –£–±–∏—Ä–∞–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –≤—ã–∑—ã–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã
        headers.pop("Sec-CH-UA", None)
        headers.pop("Sec-CH-UA-Mobile", None)
        headers.pop("Sec-CH-UA-Platform", None)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è WhatsApp
    elif "whatsapp.com" in url:
        headers["Referer"] = "https://www.google.com/"
        headers["Sec-Fetch-Site"] = "cross-site"
    
    return headers


FETCH_RETRIES = int(os.getenv("FETCH_RETRIES", "3"))
FETCH_RETRY_BACKOFF = float(os.getenv("FETCH_RETRY_BACKOFF", "1.2"))

LLM_MAX_CONCURRENCY = int(os.getenv("LLM_MAX_CONCURRENCY", "2"))
LLM_MIN_INTERVAL = float(os.getenv("LLM_MIN_INTERVAL", "0.3"))

# –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –æ—Ç —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –í–ö–õ)
PRUNE_REMOVED_SOURCES = os.getenv("PRUNE_REMOVED_SOURCES", "1") == "1"

_llm_sem = asyncio.Semaphore(LLM_MAX_CONCURRENCY)
_llm_lock = asyncio.Lock()
_last_llm_ts: float = 0.0

_SENT_SPLIT_RE = r"(?<=[\.\!\?\n])\s+"

def _split_sentences(text: str) -> List[str]:
    t = (text or "").strip()
    if not t:
        return []
    t = re.sub(r"\s+", " ", t)
    parts = re.split(_SENT_SPLIT_RE, t)
    out = []
    for p in parts:
        p = p.strip(" -‚Äì‚Äî‚Ä¢\u00a0\t")
        if len(p) >= 2:
            out.append(p)
    return out

def _pair_changed_sentences(old_sents: List[str], new_sents: List[str], threshold: float = 0.0):
    matched_pairs: List[Tuple[str, str]] = []
    old_set = set(old_sents)
    new_set = set(new_sents)
    same = old_set & new_set

    old_only = [s for s in old_sents if s not in same]
    new_only = [s for s in new_sents if s not in same]

    used_new_idx: set[int] = set()
    for s_old in old_only:
        best_j = -1
        best_score = 0.0
        for j, s_new in enumerate(new_only):
            if j in used_new_idx:
                continue
            score = SequenceMatcher(None, s_old, s_new).ratio()
            if score > best_score:
                best_score = score
                best_j = j
        if best_score > threshold and best_j >= 0:
            matched_pairs.append((s_old, new_only[best_j]))
            used_new_idx.add(best_j)

    paired_old = {w for w, _ in matched_pairs}
    paired_new = {n for _, n in matched_pairs}
    old_only_final = [s for s in old_only if s not in paired_old]
    new_only_final = [s for s in new_only if s not in paired_new]

    return matched_pairs, old_only_final, new_only_final

def _clip_line(s: str, limit: int = 800) -> str:
    s = (s or "").strip()
    if len(s) <= limit:
        return s
    return s[:limit-1].rstrip() + "‚Ä¶"


async def _summarize_async(plain: str) -> str:
    global _last_llm_ts
    async with _llm_sem:
        async with _llm_lock:
            now = time.monotonic()
            wait = LLM_MIN_INTERVAL - (now - _last_llm_ts)
            if wait > 0:
                await asyncio.sleep(wait)
            _last_llm_ts = time.monotonic()
        return await asyncio.to_thread(summarize_rules, plain)

async def run_update() -> dict:
    log.info("üîÑ Pipeline –∑–∞–ø—É—â–µ–Ω - –≤–µ—Ä—Å–∏—è 2025-10-19-v4 —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞–º–∏")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∞–ª—å–Ω–æ–≥–æ IP –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø—Ä–æ–∫—Å–∏
    try:
        # IP –±–µ–∑ –ø—Ä–æ–∫—Å–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)
        try:
            if CURL_CFFI_AVAILABLE:
                r = await _fetch_with_curl_cffi("https://httpbin.org/ip", {}, None, 5.0)
                import json
                direct_ip = json.loads(r.text).get('origin')
            else:
                async with httpx.AsyncClient(timeout=httpx.Timeout(5.0)) as client:
                    r = await client.get("https://httpbin.org/ip")
                    direct_ip = r.json().get('origin')
            log.info(f"üåé –ü—Ä—è–º–æ–π IP Railway: {direct_ip}")
        except Exception as ip_e:
            direct_ip = "unknown"
            log.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ IP: {ip_e}")
        
        # IP —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏
        if USE_PROXY:
            test_proxies = _get_proxy_for_region("GLOBAL", None, "ip_test")
            if test_proxies:
                try:
                    if CURL_CFFI_AVAILABLE:
                        r = await _fetch_with_curl_cffi("https://httpbin.org/ip", {}, test_proxies, 5.0)
                        import json
                        proxy_ip = json.loads(r.text).get('origin')
                    else:
                        async with httpx.AsyncClient(timeout=httpx.Timeout(5.0), proxies=test_proxies, verify=False) as client:
                            r = await client.get("https://httpbin.org/ip")
                            proxy_ip = r.json().get('origin')
                    
                    log.info(f"üåé IP —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏: {proxy_ip}")
                    if direct_ip == proxy_ip:
                        log.warning(f"‚ö†Ô∏è –ü–†–û–ö–°–ò –ù–ï –†–ê–ë–û–¢–ê–ï–¢! IP –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ: {direct_ip}")
                    else:
                        log.info(f"‚úÖ –ü—Ä–æ–∫—Å–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç! –ü—Ä—è–º–æ–π: {direct_ip}, –ü—Ä–æ–∫—Å–∏: {proxy_ip}")
                except Exception as proxy_e:
                    log.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–∫—Å–∏ IP: {proxy_e}")
            else:
                log.warning("‚ö†Ô∏è –ü—Ä–æ–∫—Å–∏ –∫–æ–Ω—Ñ–∏–≥ –Ω–µ –ø–æ–ª—É—á–µ–Ω!")
    except Exception as e:
        log.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ IP: {e}")
    errors: List[Dict[str, Any]] = []
    details: List[Dict[str, Any]] = []

    cache_data = load_cache() or {}
    cache: List[Dict[str, Any]] = cache_data.get("items", [])
    # –ö–ª—é—á —Ç–µ–ø–µ—Ä—å (tag, url, region)
    idx: Dict[Tuple[str, str, str], int] = {
        (it.get("tag"), it.get("url"), it.get("region", "GLOBAL")): i 
        for i, it in enumerate(cache) if isinstance(it, dict)
    }

    changed_pages = 0
    changed_sections_total = 0

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º session ID –¥–ª—è sticky-—Å–µ—Å—Å–∏–π
    session_id = f"rand{random.randint(10000, 99999)}" if PROXY_STICKY else None
    
    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –±—É–¥–µ–º —Å–æ–∑–¥–∞–≤–∞—Ç—å —Å–≤–æ–π –∫–ª–∏–µ–Ω—Ç —Å –ø–æ–¥—Ö–æ–¥—è—â–∏–º –ø—Ä–æ–∫—Å–∏
    for src_idx, src in enumerate(SOURCES):
        tag, url, title_hint = src.get("tag"), src.get("url"), src.get("title")
        region = src.get("region", "GLOBAL")
        custom_lang = src.get("lang")  # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
        proxy_country = src.get("proxy_country")  # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
        
        if not tag or not url:
            continue
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º Facebook URL –¥–ª—è –æ–±—Ö–æ–¥–∞ JavaScript —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤
        url = _fix_facebook_url(url)
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏ –¥–ª—è —Ä–µ–∑–∏–¥–µ–Ω—Ç–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏ - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        if src_idx > 0:
            if "whatsapp.com" in url:
                delay = 2.0 + random.random() * 1.0  # 2-3 —Å–µ–∫ –¥–ª—è WhatsApp
                log.info(f"üí¨ ‚è≥ WhatsApp: {delay:.1f} —Å–µ–∫")
            else:
                # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã —Å –ø—Ä–æ–∫—Å–∏ - –∫–∞–∫ –Ω–∞ –ª–æ–∫–∞–ª–∫–µ
                delay = 0.5 + random.random() * 1.0  # 0.5-1.5 —Å–µ–∫ –¥–ª—è Meta
                log.info(f"‚è≥ {delay:.1f} —Å–µ–∫")
            await asyncio.sleep(delay)
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–∫—Å–∏ –¥–ª—è —Ä–µ–≥–∏–æ–Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        proxies = _get_proxy_for_region(region, proxy_country, session_id)
        
        # –û—Ç–ª–∞–¥–æ—á–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∫—Å–∏
        if proxies:
            proxy_url = proxies.get('https://') or proxies.get('http://', '')
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ proxy.froxy.com:9000 –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
            safe_proxy = proxy_url.split('@')[-1] if '@' in proxy_url else proxy_url
            log.info(f"üîê –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–∫—Å–∏ –¥–ª—è {region}: {safe_proxy}")
        else:
            log.warning(f"‚ö†Ô∏è –ü—Ä–æ–∫—Å–∏ –ù–ï –ò–°–ü–û–õ–¨–ó–£–ï–¢–°–Ø –¥–ª—è {region} (proxies=None)")
        
        # Accept-Language –ø–æ —Ä–µ–≥–∏–æ–Ω—É –∏–ª–∏ –∫–∞—Å—Ç–æ–º–Ω—ã–π
        accept_lang = custom_lang or _DEFAULT_LANG_BY_REGION.get(region, "en-US,en;q=0.9")
        headers = _get_random_headers(url, accept_lang)
        
        # SSL –ø—Ä–æ–≤–µ—Ä–∫–∞: –æ—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–∏ –ø—Ä–æ–∫—Å–∏
        verify_ssl = proxies is None
        
        html = None
        used_fallback = False
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º curl-cffi –¥–ª—è –í–°–ï–• –∑–∞–ø—Ä–æ—Å–æ–≤ —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏
        use_curl_cffi = CURL_CFFI_AVAILABLE and USE_PROXY
        
        try:
            # Retry –ª–æ–≥–∏–∫–∞
            err = None
            for attempt in range(FETCH_RETRIES):
                try:
                    log.info(f"üîç HTTP –∑–∞–ø—Ä–æ—Å attempt {attempt+1}/{FETCH_RETRIES} –∫ {url} ({'curl-cffi' if use_curl_cffi else 'httpx'})")
                    
                    if use_curl_cffi:
                        timeout_seconds = TIMEOUT.total if hasattr(TIMEOUT, 'total') else 30.0
                        r = await _fetch_with_curl_cffi(url, headers, proxies, timeout_seconds)
                    else:
                        async with httpx.AsyncClient(timeout=TIMEOUT, follow_redirects=True, proxies=proxies, verify=verify_ssl) as client:
                            r = await client.get(url, headers=headers)
                    
                    log.info(f"üîç HTTP –æ—Ç–≤–µ—Ç: —Å—Ç–∞—Ç—É—Å {r.status_code}, HTML: {len(r.text)} —Å–∏–º–≤")
                    
                    # –û—Å–æ–±–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Å—Ç–∞—Ç—É—Å–æ–≤ 400/422 - Meta —Å–∞–π—Ç—ã —á–∞—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç —ç—Ç–∏ –∫–æ–¥—ã —Å –≤–∞–ª–∏–¥–Ω—ã–º HTML
                    log.debug(f"üîç DEBUG: –ü–æ–ª—É—á–∏–ª–∏ —Å—Ç–∞—Ç—É—Å {r.status_code} –¥–ª—è {url}")
                    if r.status_code in [400, 422]:
                        # –î–ª—è Meta/Facebook —Å–∞–π—Ç–æ–≤ –ø—Ä–∏–Ω–∏–º–∞–µ–º –ª—é–±–æ–π –æ—Ç–≤–µ—Ç —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º
                        is_meta_site = any(domain in url for domain in ["transparency.meta.com", "facebook.com", "about.fb.com", "developers.facebook.com"])
                        log.info(f"üîç {r.status_code} DEBUG: is_meta_site={is_meta_site}, HTML size={len(r.text) if r.text else 0}")
                        if is_meta_site and r.text and len(r.text.strip()) > 100:
                            log.info(f"‚úÖ Meta —Å–∞–π—Ç: –°—Ç–∞—Ç—É—Å {r.status_code} –Ω–æ –ø–æ–ª—É—á–µ–Ω HTML ({len(r.text)} —Å–∏–º–≤.), –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º")
                            html = r.text
                        elif r.text and len(r.text.strip()) > 500:
                            log.info(f"‚úÖ –°—Ç–∞—Ç—É—Å {r.status_code} –Ω–æ –ø–æ–ª—É—á–µ–Ω –≤–∞–ª–∏–¥–Ω—ã–π HTML ({len(r.text)} —Å–∏–º–≤.), –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º")
                            html = r.text
                        else:
                            log.warning(f"‚ö†Ô∏è –°—Ç–∞—Ç—É—Å {r.status_code} —Å –∫–æ—Ä–æ—Ç–∫–∏–º –æ—Ç–≤–µ—Ç–æ–º ({len(r.text) if r.text else 0} —Å–∏–º–≤.), –ø–æ–ø—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑")
                            # –ù–ï –≤—ã–∑—ã–≤–∞–µ–º raise_for_status –¥–ª—è 400/422 - –ø—É—Å—Ç—å retry —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç
                            continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç attempt –∏ –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π
                    elif r.status_code in [200, 201, 202]:
                        html = r.text
                    else:
                        r.raise_for_status()
                        html = r.text
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
                    if "You're Temporarily Blocked" in html or "going too fast" in html:
                        if hasattr(r, 'request'):
                            raise httpx.HTTPStatusError("Temporary block", request=r.request, response=r)
                        else:
                            raise Exception("Temporary block detected")
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ JavaScript —Ä–µ–¥–∏—Ä–µ–∫—Ç
                    if 'http-equiv="refresh"' in html and '_fb_noscript=1' in html:
                        # –ü–æ–ª—É—á–∏–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–º, –Ω–æ —É–∂–µ —Å _fb_noscript - —ç—Ç–æ –æ—à–∏–±–∫–∞
                        if hasattr(r, 'request'):
                            raise httpx.HTTPStatusError("JS redirect page despite _fb_noscript=1", request=r.request, response=r)
                        else:
                            raise Exception("JS redirect page despite _fb_noscript=1")
                    
                    elif 'http-equiv="refresh"' in html and 'URL=' in html:
                        # –û–±–Ω–∞—Ä—É–∂–µ–Ω JavaScript —Ä–µ–¥–∏—Ä–µ–∫—Ç, –ø–æ–ø—Ä–æ–±—É–µ–º —Å _fb_noscript=1
                        import re
                        redirect_match = re.search(r'URL=([^"]+)', html)
                        if redirect_match:
                            redirect_url = redirect_match.group(1)
                            if not redirect_url.startswith('http'):
                                # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π URL
                                from urllib.parse import urljoin
                                redirect_url = urljoin(url, redirect_url)
                            
                            log.info(f"üîÑ JS —Ä–µ–¥–∏—Ä–µ–∫—Ç –æ–±–Ω–∞—Ä—É–∂–µ–Ω, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞: {redirect_url}")
                            
                            if use_curl_cffi:
                                timeout_seconds = TIMEOUT.total if hasattr(TIMEOUT, 'total') else 30.0
                                r = await _fetch_with_curl_cffi(redirect_url, headers, proxies, timeout_seconds)
                            else:
                                # –î–ª—è httpx —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∫–ª–∏–µ–Ω—Ç
                                async with httpx.AsyncClient(timeout=TIMEOUT, follow_redirects=True, proxies=proxies, verify=verify_ssl) as redirect_client:
                                    r = await redirect_client.get(redirect_url, headers=headers)
                                    r.raise_for_status()
                            html = r.text
                    
                    break  # –£—Å–ø–µ—à–Ω–æ!
                except (httpx.HTTPStatusError, httpx.ProxyError, Exception) as e:
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –¥–ª—è –æ–±–æ–∏—Ö httpx –∏ curl-cffi
                    if isinstance(e, httpx.ProxyError):
                        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —Å—Ç–∞—Ç—É—Å –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è
                        error_msg = str(e)
                        if '400' in error_msg:
                            status = 400
                        elif '422' in error_msg:
                            status = 422
                        elif '500' in error_msg:
                            status = 500
                        else:
                            status = 0
                        response_text = ''
                        log.info(f"üîç ProxyError: —Å–æ–æ–±—â–µ–Ω–∏–µ='{error_msg}', –∏–∑–≤–ª–µ—á–µ–Ω —Å—Ç–∞—Ç—É—Å={status}")
                    elif isinstance(e, httpx.HTTPStatusError):
                        status = getattr(e.response, 'status_code', 0) if hasattr(e, 'response') else 0
                        response_text = getattr(e.response, 'text', '') if hasattr(e, 'response') and e.response else ''
                        log.info(f"üîç {type(e).__name__} –ø–æ–π–º–∞–Ω: —Å—Ç–∞—Ç—É—Å {status}, HTML: {len(response_text)} —Å–∏–º–≤")
                    else:
                        # –û—à–∏–±–∫–∏ curl-cffi –∏–ª–∏ –¥—Ä—É–≥–∏–µ
                        status = 0
                        response_text = ''
                        log.info(f"üîç {type(e).__name__}: {str(e)}")
                    
                    # curl-cffi –¥–æ–ª–∂–µ–Ω —Ä–µ—à–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—É TLS fingerprinting
                    if use_curl_cffi:
                        log.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å curl-cffi: {e}")
                    
                    # 407/403 –¥–ª—è MD -> –ø—Ä–æ–±—É–µ–º fallback –Ω–∞ EU
                    if status in (407, 403) and region == "MD" and PROXY_FALLBACK_EU and PROXY_URL_EU and attempt == 0:
                        log.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ {status} –¥–ª—è MD –ø—Ä–æ–∫—Å–∏, –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ EU fallback...")
                        # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ EU –ø—Ä–æ–∫—Å–∏
                        proxies = _get_proxy_for_region("EU", proxy_country, session_id)
                        used_fallback = True
                        await asyncio.sleep(2)  # –ë—ã—Å—Ç—Ä–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ EU
                        continue
                    
                    if status in (500, 502, 503, 429, 403, 407):
                        err = e
                        if attempt < FETCH_RETRIES - 1:
                            backoff = FETCH_RETRY_BACKOFF * (1.5 ** attempt) + random.random() * 2  # –ë—ã—Å—Ç—Ä—ã–µ retry
                            if status == 500:
                                log.warning(f"‚ö†Ô∏è –°–µ—Ä–≤–µ—Ä Meta –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (500), –ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/{FETCH_RETRIES}, –æ–∂–∏–¥–∞–Ω–∏–µ {backoff:.1f} —Å–µ–∫...")
                            else:
                                log.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ {status} –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}, –ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/{FETCH_RETRIES}, –æ–∂–∏–¥–∞–Ω–∏–µ {backoff:.1f} —Å–µ–∫...")
                            await asyncio.sleep(backoff)
                            headers = _get_random_headers(url, accept_lang)
                        else:
                            if status == 429:
                                log.error(f"‚ùå Facebook –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –∑–∞–ø—Ä–æ—Å—ã: {url}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
                                err = None
                                break
                            raise
                    else:
                        raise
                else:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å–ª–∏ HTML –ø–æ–ª—É—á–µ–Ω —á–µ—Ä–µ–∑ fallback - –Ω–µ –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –æ—à–∏–±–∫—É
                    if html:
                        log.info(f"‚úÖ HTML –ø–æ–ª—É—á–µ–Ω —á–µ—Ä–µ–∑ fallback, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É")
                        err = None  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –æ—à–∏–±–∫—É
                    elif err:
                        raise err
        except Exception as e:
            log.info(f"üîç –í–Ω–µ—à–Ω–∏–π Exception –ø–æ–π–º–∞–Ω: {type(e).__name__}: {e}, HTML: {len(html) if html else 0} —Å–∏–º–≤")
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–µ –ø–æ–ª—É—á–∏–ª–∏ –ª–∏ HTML –≤–æ –≤—Ä–µ–º—è 422 –æ—à–∏–±–∫–∏
            if html:
                log.info(f"‚úÖ HTML –ø–æ–ª—É—á–µ–Ω –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –æ—à–∏–±–∫—É ({len(html)} —Å–∏–º–≤.), –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É")
            else:
                log.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ %s: %s", url, e)
                errors.append({"tag": tag, "url": url, "region": region, "error": str(e)})
                continue
        
        if not html:
            errors.append({"tag": tag, "url": url, "region": region, "error": "No HTML received"})
            continue
        
        if used_fallback:
            log.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω–æ —á–µ—Ä–µ–∑ EU fallback: {url}")
        
        title_auto, full_plain, cleaned_html = clean_html(html, url)

        plain_norm = normalize_plain(full_plain or "")
        page_sig = compute_hash(plain_norm)

        sections_new = extract_sections(cleaned_html or html)
        sec_map_new = {s["id"]: s for s in sections_new if s.get("id")}

        key = (tag, url, region)
        existing_i = idx.get(key)
        existing = cache[existing_i] if existing_i is not None else None

        added_ids, removed_ids, modified_ids = [], [], []

        if existing:
            old_sections = existing.get("sections") or []
            sec_map_old = {s.get("id"): s for s in old_sections if s.get("id")}
            new_ids = set(sec_map_new.keys())
            old_ids = set(sec_map_old.keys())
            added_ids = list(new_ids - old_ids)
            removed_ids = list(old_ids - new_ids)
            modified_ids = [sid for sid in (new_ids & old_ids)
                            if sec_map_new[sid].get("sig") != sec_map_old[sid].get("sig")]
        else:
            added_ids = list(sec_map_new.keys())

        changed_here = bool(
            added_ids or removed_ids or modified_ids or
            (existing is None) or
            (existing and existing.get("hash") != page_sig)
        )
        if not changed_here:
            continue
        
        if page_sig in trans_cache:
            summary = trans_cache[page_sig]
        else:
            summary = await _summarize_async(full_plain or "")
            trans_cache[page_sig] = summary
        
        title = (title_hint or title_auto or "").strip() or url
        
        old_full = (existing or {}).get("full_text") or ""
        new_full = full_plain or ""
        old_sents = _split_sentences(old_full)
        new_sents = _split_sentences(new_full)
        pairs_global, old_only_global, new_only_global = _pair_changed_sentences(
            old_sents, new_sents, threshold=0.0
        )

        global_diff = {
            "changed": [{"was": _clip_line(w), "now": _clip_line(n)} for (w, n) in pairs_global],
            "removed": [_clip_line(s) for s in old_only_global],
            "added": [_clip_line(s) for s in new_only_global],
        }

        section_diffs: List[Dict[str, Any]] = []
        if added_ids:
            added_preview = []
            for sid in added_ids:
                sents = _split_sentences(sec_map_new[sid].get("text") or "")
                added_preview.append(_clip_line(sents[0] if sents else (sec_map_new[sid].get("title") or sid)))
            section_diffs.append({"type": "added", "title": "–î–æ–±–∞–≤–ª–µ–Ω–æ", "added": added_preview})

        if removed_ids:
            removed_titles = []
            for sid in removed_ids:
                old_sec = next((s for s in (existing or {}).get("sections", []) if s.get("id") == sid), None)
                ttl = (old_sec or {}).get("title") or sid
                removed_titles.append(_clip_line(ttl))
            section_diffs.append({"type": "removed", "title": "–£–¥–∞–ª–µ–Ω–æ", "removed": removed_titles})

        if modified_ids:
            for sid in modified_ids:
                old_s = next((s for s in (existing or {}).get("sections", []) if s.get("id") == sid), {})
                new_s = sec_map_new[sid]
                old_txt = old_s.get("text") or ""
                new_txt = new_s.get("text") or ""
                old_sents_s = _split_sentences(old_txt)
                new_sents_s = _split_sentences(new_txt)
                pairs_s, old_only_s, new_only_s = _pair_changed_sentences(
                    old_sents_s, new_sents_s, threshold=0.0
                )
                block = {
                    "type": "changed",
                    "title": new_s.get("title") or sid,
                    "changed": [{"was": _clip_line(w), "now": _clip_line(n)} for (w, n) in pairs_s]
                }
                if old_only_s:
                    block["removed_inline"] = [_clip_line(s) for s in old_only_s]
                if new_only_s:
                    block["added_inline"] = [_clip_line(s) for s in new_only_s]
                section_diffs.append(block)

        item = {
            "tag": tag,
            "url": url,
            "region": region,  # ‚ú® –¥–æ–±–∞–≤–ª–µ–Ω region
            "title": title,
            "summary": (summary or "").strip(),
            "ts": datetime.now(timezone.utc).isoformat(timespec="seconds"),
            "hash": page_sig,
            "sections": sections_new,
            "full_text": new_full,
            "last_changed_at": datetime.now(timezone.utc).isoformat(timespec="seconds"),
        }

        if existing_i is not None:
            cache[existing_i] = item
        else:
            cache.append(item)
            idx[key] = len(cache) - 1
        
        changed_pages += 1
        changed_sections_total += len(added_ids) + len(modified_ids) + len(removed_ids)
        
        details.append({
            "tag": tag,
            "url": url,
            "region": region,  # ‚ú® –¥–æ–±–∞–≤–ª–µ–Ω region
            "title": title,
            "diff": {
                "added": [sec_map_new[sid].get("title") or sid for sid in added_ids],
                "modified": [sec_map_new[sid].get("title") or sid for sid in modified_ids],
                "removed": [
                    (next((s.get("title") for s in (existing or {}).get("sections", [])
                           if s.get("id") == sid), sid))
                    for sid in removed_ids
                ],
            },
            "global_diff": global_diff,
            "section_diffs": section_diffs
        })

    # üîß –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —á–∏—Å—Ç–∏–º –∫—ç—à –æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –±–æ–ª—å—à–µ –Ω–µ—Ç –≤ config.json
    if PRUNE_REMOVED_SOURCES:
        # –ö–ª—é—á —Ç–µ–ø–µ—Ä—å (tag, url, region)
        valid_tuples = {(s.get("tag"), s.get("url"), s.get("region", "GLOBAL")) for s in SOURCES if s.get("tag") and s.get("url")}
        cache = [it for it in cache if (it.get("tag"), it.get("url"), it.get("region", "GLOBAL")) in valid_tuples]

    stats = get_cache_stats()
    cache.sort(key=lambda x: x.get("ts", ""), reverse=True)
    if stats.get("max_cache") and len(cache) > stats["max_cache"]:
        cache = cache[:stats["max_cache"]]

    cache_data["items"] = cache
    save_cache(cache_data)

    try:
        tmp = TRANS_CACHE_FILE.with_suffix(".tmp")
        with open(tmp, "w", encoding="utf-8") as tf:
            json.dump(trans_cache, tf, ensure_ascii=False, indent=2)
        os.replace(tmp, TRANS_CACHE_FILE)
    except Exception as e:
        log.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à –ø–µ—Ä–µ–≤–æ–¥–æ–≤: %s", e)

    return {
        "changed": changed_pages,
        "errors": errors,
        "sections_total_changed": changed_sections_total,
        "details": details,
    }

def get_stats() -> dict:
    return get_cache_stats()
