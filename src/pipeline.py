# -*- coding: utf-8 -*-
import logging
import os
import json
import time
import random
from datetime import datetime, timezone
from typing import Dict, List, Tuple, Any, Optional

import asyncio
import httpx
from difflib import SequenceMatcher
import re

from .storage import load_cache, save_cache, compute_hash, get_cache_stats
from .config import PROJECT_ROOT, SOURCES
from .html_clean import clean_html
from .summarize import summarize_rules, normalize_plain, extract_sections

log = logging.getLogger(__name__)

TRANS_CACHE_FILE = PROJECT_ROOT / "data" / "trans_cache.json"
if TRANS_CACHE_FILE.exists():
    try:
        trans_cache: Dict[str, str] = json.loads(TRANS_CACHE_FILE.read_text(encoding="utf-8"))
    except Exception:
        trans_cache = {}
else:
    trans_cache = {}

TIMEOUT = httpx.Timeout(30.0, connect=15.0)  # –£–≤–µ–ª–∏—á–∏–ª–∏ timeout

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–∫—Å–∏ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
PROXY_HOST = os.getenv("PROXY_HOST", "")
PROXY_USER = os.getenv("PROXY_USER", "")
PROXY_PASSWORD = os.getenv("PROXY_PASSWORD", "")

def _get_proxy_config(session_id: Optional[str] = None) -> Optional[Dict[str, str]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–∫—Å–∏ –¥–ª—è httpx, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã.
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - session_id: –ò–î —Å–µ—Å—Å–∏–∏ –¥–ª—è BrightData (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–¥–∏–Ω IP –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤)
    """
    if PROXY_HOST and PROXY_USER and PROXY_PASSWORD:
        proxy_user = PROXY_USER
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–≤—è–∑–∫—É –∫ —Å—Ç—Ä–∞–Ω–µ –∏ session ID
        if "-country-" not in proxy_user:
            # –ï—Å–ª–∏ –Ω–µ—Ç —É–∫–∞–∑–∞–Ω–∏—è —Å—Ç—Ä–∞–Ω—ã - –¥–æ–±–∞–≤–ª—è–µ–º TARGET_REGION
            country_code = TARGET_REGION.lower() if TARGET_REGION != "AUTO" else "md"
            proxy_user = f"{proxy_user}-country-{country_code}"
        
        # –î–æ–±–∞–≤–ª—è–µ–º session ID –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–¥–Ω–æ–≥–æ IP
        if session_id and "-session-" not in proxy_user:
            proxy_user = f"{proxy_user}-session-{session_id}"
        
        proxy_url = f"http://{proxy_user}:{PROXY_PASSWORD}@{PROXY_HOST}"
        log.info(f"üîê –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–∫—Å–∏: {PROXY_HOST} (—Å—Ç—Ä–∞–Ω–∞: {TARGET_REGION}, session: {session_id or '–Ω–µ—Ç'})")
        return {"http://": proxy_url, "https://": proxy_url}
    else:
        log.warning("‚ö†Ô∏è –ü—Ä–æ–∫—Å–∏ –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω, –∑–∞–ø—Ä–æ—Å—ã –∏–¥—É—Ç –Ω–∞–ø—Ä—è–º—É—é")
        return None

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–µ–≥–∏–æ–Ω–∞ (–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è)
TARGET_REGION = os.getenv("TARGET_REGION", "MD")  # MD=Moldova (EU), US=United States

# –Ø–∑—ã–∫–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º
REGION_SETTINGS = {
    "MD": {
        "lang": "en-GB,en;q=0.9,ro;q=0.8,ru;q=0.7",  # English (UK/EU), Romanian, Russian
        "country": "MD",
        "timezone": "Europe/Chisinau"
    },
    "US": {
        "lang": "en-US,en;q=0.9",
        "country": "US",
        "timezone": "America/New_York"
    },
    "EU": {
        "lang": "en-GB,en;q=0.9",
        "country": "GB",  # UK as EU representative
        "timezone": "Europe/London"
    }
}

region_config = REGION_SETTINGS.get(TARGET_REGION, REGION_SETTINGS["MD"])

# –†–æ—Ç–∞—Ü–∏—è User-Agent –¥–ª—è –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:132.0) Gecko/20100101 Firefox/132.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.6 Safari/605.1.15",
]

def _get_random_headers(url: str = ""):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
    ua = random.choice(USER_AGENTS)
    headers = {
        "User-Agent": ua,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": region_config["lang"],
        "Cache-Control": "max-age=0",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Sec-Fetch-User": "?1",
        "Sec-CH-UA": '"Not_A Brand";v="8", "Chromium";v="131"',
        "Sec-CH-UA-Mobile": "?0",
        "Sec-CH-UA-Platform": '"Windows"',
    }
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è WhatsApp
    if "whatsapp.com" in url:
        headers["Referer"] = "https://www.google.com/"
        headers["Sec-Fetch-Site"] = "cross-site"
        # –≠–º—É–ª–∏—Ä—É–µ–º –ø–µ—Ä–µ—Ö–æ–¥ —Å Google
    
    return headers


FETCH_RETRIES = int(os.getenv("FETCH_RETRIES", "3"))
FETCH_RETRY_BACKOFF = float(os.getenv("FETCH_RETRY_BACKOFF", "1.2"))

LLM_MAX_CONCURRENCY = int(os.getenv("LLM_MAX_CONCURRENCY", "2"))
LLM_MIN_INTERVAL = float(os.getenv("LLM_MIN_INTERVAL", "0.3"))

# –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –æ—Ç —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –í–ö–õ)
PRUNE_REMOVED_SOURCES = os.getenv("PRUNE_REMOVED_SOURCES", "1") == "1"

_llm_sem = asyncio.Semaphore(LLM_MAX_CONCURRENCY)
_llm_lock = asyncio.Lock()
_last_llm_ts: float = 0.0

_SENT_SPLIT_RE = r"(?<=[\.\!\?\n])\s+"

def _split_sentences(text: str) -> List[str]:
    t = (text or "").strip()
    if not t:
        return []
    t = re.sub(r"\s+", " ", t)
    parts = re.split(_SENT_SPLIT_RE, t)
    out = []
    for p in parts:
        p = p.strip(" -‚Äì‚Äî‚Ä¢\u00a0\t")
        if len(p) >= 2:
            out.append(p)
    return out

def _pair_changed_sentences(old_sents: List[str], new_sents: List[str], threshold: float = 0.0):
    matched_pairs: List[Tuple[str, str]] = []
    old_set = set(old_sents)
    new_set = set(new_sents)
    same = old_set & new_set

    old_only = [s for s in old_sents if s not in same]
    new_only = [s for s in new_sents if s not in same]

    used_new_idx: set[int] = set()
    for s_old in old_only:
        best_j = -1
        best_score = 0.0
        for j, s_new in enumerate(new_only):
            if j in used_new_idx:
                continue
            score = SequenceMatcher(None, s_old, s_new).ratio()
            if score > best_score:
                best_score = score
                best_j = j
        if best_score > threshold and best_j >= 0:
            matched_pairs.append((s_old, new_only[best_j]))
            used_new_idx.add(best_j)

    paired_old = {w for w, _ in matched_pairs}
    paired_new = {n for _, n in matched_pairs}
    old_only_final = [s for s in old_only if s not in paired_old]
    new_only_final = [s for s in new_only if s not in paired_new]

    return matched_pairs, old_only_final, new_only_final

def _clip_line(s: str, limit: int = 800) -> str:
    s = (s or "").strip()
    if len(s) <= limit:
        return s
    return s[:limit-1].rstrip() + "‚Ä¶"


async def _summarize_async(plain: str) -> str:
    global _last_llm_ts
    async with _llm_sem:
        async with _llm_lock:
            now = time.monotonic()
            wait = LLM_MIN_INTERVAL - (now - _last_llm_ts)
            if wait > 0:
                await asyncio.sleep(wait)
            _last_llm_ts = time.monotonic()
        return await asyncio.to_thread(summarize_rules, plain)

async def run_update() -> dict:
    errors: List[Dict[str, Any]] = []
    details: List[Dict[str, Any]] = []

    cache_data = load_cache() or {}
    cache: List[Dict[str, Any]] = cache_data.get("items", [])
    idx: Dict[Tuple[str, str], int] = {(it.get("tag"), it.get("url")): i for i, it in enumerate(cache) if isinstance(it, dict)}

    changed_pages = 0
    changed_sections_total = 0

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º session ID –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–¥–Ω–æ–≥–æ IP –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    session_id = f"session_{int(time.time())}"
    
    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–∫—Å–∏ —Å session ID
    proxies = _get_proxy_config(session_id)
    
    # –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É SSL –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–∫—Å–∏ (BrightData –∏—Å–ø–æ–ª—å–∑—É–µ—Ç MITM)
    verify_ssl = proxies is None
    
    async with httpx.AsyncClient(timeout=TIMEOUT, follow_redirects=True, proxies=proxies, verify=verify_ssl) as client:
        for src_idx, src in enumerate(SOURCES):
            tag, url, title_hint = src.get("tag"), src.get("url"), src.get("title")
            if not tag or not url:
                continue

            # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫ "going too fast"
            if src_idx > 0:
                # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏ –¥–ª—è WhatsApp (–±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–µ –ª–∏–º–∏—Ç—ã)
                if "whatsapp.com" in url:
                    delay = 45.0 + random.random() * 15.0  # 45-60 —Å–µ–∫ –¥–ª—è WhatsApp
                    log.info(f"üí¨ ‚è≥ WhatsApp: –æ–∂–∏–¥–∞–Ω–∏–µ {delay:.1f} —Å–µ–∫ (—É–≤–µ–ª–∏—á–µ–Ω–Ω–∞—è –ø–∞—É–∑–∞)...")
                else:
                    # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏ –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ (–∏–∑–±–µ–≥–∞–µ–º "going too fast")
                    if random.random() < 0.5:
                        delay = 20.0 + random.random() * 10.0  # 20-30 —Å–µ–∫
                    else:
                        delay = 30.0 + random.random() * 10.0  # 30-40 —Å–µ–∫
                    log.info(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {delay:.1f} —Å–µ–∫ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –∑–∞–ø—Ä–æ—Å–æ–º...")
                await asyncio.sleep(delay)

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ (—Å —É—á–µ—Ç–æ–º URL)
            headers = _get_random_headers(url)
            
            try:
                # Retry –ª–æ–≥–∏–∫–∞ —Å —Ä–∞–∑–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π 502
                err = None
                for attempt in range(FETCH_RETRIES):
                    try:
                        r = await client.get(url, headers=headers)
                        r.raise_for_status()
                        html = r.text
                        
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É Facebook (–≤—Ä–µ–º–µ–Ω–Ω–∞—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞)
                        if "You're Temporarily Blocked" in html or "going too fast" in html:
                            # –°–æ–∑–¥–∞–µ–º –ø—Å–µ–≤–¥–æ-response –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
                            class TempBlockResponse:
                                status_code = 429
                                request = r.request
                            
                            class TempBlockError(httpx.HTTPStatusError):
                                def __init__(self):
                                    self.response = TempBlockResponse()
                                    super().__init__("Temporary block detected", request=r.request, response=self.response)
                            
                            raise TempBlockError()
                        
                        break  # –£—Å–ø–µ—à–Ω–æ!
                    except httpx.HTTPStatusError as e:
                        # –û–±—Ä–∞–±–æ—Ç–∫–∞ 502, 403 –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
                        status = getattr(e.response, 'status_code', 0) if hasattr(e, 'response') else 0
                        if status in (502, 503, 429, 403) or "Temporary block" in str(e):
                            err = e
                            if attempt < FETCH_RETRIES - 1:
                                # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø—Ä–∏ 502 –∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞—Ö
                                backoff = FETCH_RETRY_BACKOFF * (3 ** attempt) + random.random() * 5
                                log.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ {status} –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}, –ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/{FETCH_RETRIES}, –æ–∂–∏–¥–∞–Ω–∏–µ {backoff:.1f} —Å–µ–∫...")
                                await asyncio.sleep(backoff)
                                # –ú–µ–Ω—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–∏
                                headers = _get_random_headers(url)
                            else:
                                # –ï—Å–ª–∏ —ç—Ç–æ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ Facebook - –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                                if status == 429:
                                    log.error(f"‚ùå Facebook –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –∑–∞–ø—Ä–æ—Å—ã: {url}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
                                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç URL, –Ω–µ –ø–∞–¥–∞–µ–º
                                    err = None
                                    break
                                raise
                        else:
                            raise
                    except Exception as e:
                        err = e
                        if attempt < FETCH_RETRIES - 1:
                            backoff = FETCH_RETRY_BACKOFF * (2 ** attempt)
                            await asyncio.sleep(backoff)
                            # –ú–µ–Ω—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–∏
                            headers = _get_random_headers(url)
                        else:
                            raise
                else:
                    # –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã
                    if err:
                        raise err
                    # –ï—Å–ª–∏ err = None, –∑–Ω–∞—á–∏—Ç –º—ã –ø—Ä–æ–ø—É—Å—Ç–∏–ª–∏ URL –∏–∑-–∑–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
            except Exception as e:
                log.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ %s: %s", url, e)
                errors.append({"tag": tag, "url": url, "error": str(e)})
                continue
            
            # –ï—Å–ª–∏ err = None –∏ –º—ã –ø—Ä–æ–ø—É—Å—Ç–∏–ª–∏ URL - –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É
            if err is None or not locals().get('html'):
                errors.append({"tag": tag, "url": url, "error": "Facebook temporary block"})
                continue

            title_auto, full_plain, cleaned_html = clean_html(html, url)

            plain_norm = normalize_plain(full_plain or "")
            page_sig = compute_hash(plain_norm)

            sections_new = extract_sections(cleaned_html or html)
            sec_map_new = {s["id"]: s for s in sections_new if s.get("id")}

            key = (tag, url)
            existing_i = idx.get(key)
            existing = cache[existing_i] if existing_i is not None else None

            added_ids, removed_ids, modified_ids = [], [], []

            if existing:
                old_sections = existing.get("sections") or []
                sec_map_old = {s.get("id"): s for s in old_sections if s.get("id")}
                new_ids = set(sec_map_new.keys())
                old_ids = set(sec_map_old.keys())
                added_ids = list(new_ids - old_ids)
                removed_ids = list(old_ids - new_ids)
                modified_ids = [sid for sid in (new_ids & old_ids)
                                if sec_map_new[sid].get("sig") != sec_map_old[sid].get("sig")]
            else:
                added_ids = list(sec_map_new.keys())

            changed_here = bool(
                added_ids or removed_ids or modified_ids or
                (existing is None) or
                (existing and existing.get("hash") != page_sig)
            )
            if not changed_here:
                continue

            if page_sig in trans_cache:
                summary = trans_cache[page_sig]
            else:
                summary = await _summarize_async(full_plain or "")
                trans_cache[page_sig] = summary

            title = (title_hint or title_auto or "").strip() or url

            old_full = (existing or {}).get("full_text") or ""
            new_full = full_plain or ""
            old_sents = _split_sentences(old_full)
            new_sents = _split_sentences(new_full)
            pairs_global, old_only_global, new_only_global = _pair_changed_sentences(
                old_sents, new_sents, threshold=0.0
            )

            global_diff = {
                "changed": [{"was": _clip_line(w), "now": _clip_line(n)} for (w, n) in pairs_global],
                "removed": [_clip_line(s) for s in old_only_global],
                "added": [_clip_line(s) for s in new_only_global],
            }

            section_diffs: List[Dict[str, Any]] = []
            if added_ids:
                added_preview = []
                for sid in added_ids:
                    sents = _split_sentences(sec_map_new[sid].get("text") or "")
                    added_preview.append(_clip_line(sents[0] if sents else (sec_map_new[sid].get("title") or sid)))
                section_diffs.append({"type": "added", "title": "–î–æ–±–∞–≤–ª–µ–Ω–æ", "added": added_preview})

            if removed_ids:
                removed_titles = []
                for sid in removed_ids:
                    old_sec = next((s for s in (existing or {}).get("sections", []) if s.get("id") == sid), None)
                    ttl = (old_sec or {}).get("title") or sid
                    removed_titles.append(_clip_line(ttl))
                section_diffs.append({"type": "removed", "title": "–£–¥–∞–ª–µ–Ω–æ", "removed": removed_titles})

            if modified_ids:
                for sid in modified_ids:
                    old_s = next((s for s in (existing or {}).get("sections", []) if s.get("id") == sid), {})
                    new_s = sec_map_new[sid]
                    old_txt = old_s.get("text") or ""
                    new_txt = new_s.get("text") or ""
                    old_sents_s = _split_sentences(old_txt)
                    new_sents_s = _split_sentences(new_txt)
                    pairs_s, old_only_s, new_only_s = _pair_changed_sentences(
                        old_sents_s, new_sents_s, threshold=0.0
                    )
                    block = {
                        "type": "changed",
                        "title": new_s.get("title") or sid,
                        "changed": [{"was": _clip_line(w), "now": _clip_line(n)} for (w, n) in pairs_s]
                    }
                    if old_only_s:
                        block["removed_inline"] = [_clip_line(s) for s in old_only_s]
                    if new_only_s:
                        block["added_inline"] = [_clip_line(s) for s in new_only_s]
                    section_diffs.append(block)

            item = {
                "tag": tag,
                "url": url,
                "title": title,
                "summary": (summary or "").strip(),
                "ts": datetime.now(timezone.utc).isoformat(timespec="seconds"),
                "hash": page_sig,
                "sections": sections_new,
                "full_text": new_full,
                "last_changed_at": datetime.now(timezone.utc).isoformat(timespec="seconds"),
            }

            if existing_i is not None:
                cache[existing_i] = item
            else:
                cache.append(item)
                idx[key] = len(cache) - 1

            changed_pages += 1
            changed_sections_total += len(added_ids) + len(modified_ids) + len(removed_ids)

            details.append({
                "tag": tag,
                "url": url,
                "title": title,
                "diff": {
                    "added": [sec_map_new[sid].get("title") or sid for sid in added_ids],
                    "modified": [sec_map_new[sid].get("title") or sid for sid in modified_ids],
                    "removed": [
                        (next((s.get("title") for s in (existing or {}).get("sections", [])
                               if s.get("id") == sid), sid))
                        for sid in removed_ids
                    ],
                },
                "global_diff": global_diff,
                "section_diffs": section_diffs
            })

    # üîß –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —á–∏—Å—Ç–∏–º –∫—ç—à –æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –±–æ–ª—å—à–µ –Ω–µ—Ç –≤ config.json
    if PRUNE_REMOVED_SOURCES:
        valid_pairs = {(s.get("tag"), s.get("url")) for s in SOURCES if s.get("tag") and s.get("url")}
        cache = [it for it in cache if (it.get("tag"), it.get("url")) in valid_pairs]

    stats = get_cache_stats()
    cache.sort(key=lambda x: x.get("ts", ""), reverse=True)
    if stats.get("max_cache") and len(cache) > stats["max_cache"]:
        cache = cache[:stats["max_cache"]]

    cache_data["items"] = cache
    save_cache(cache_data)

    try:
        tmp = TRANS_CACHE_FILE.with_suffix(".tmp")
        with open(tmp, "w", encoding="utf-8") as tf:
            json.dump(trans_cache, tf, ensure_ascii=False, indent=2)
        os.replace(tmp, TRANS_CACHE_FILE)
    except Exception as e:
        log.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à –ø–µ—Ä–µ–≤–æ–¥–æ–≤: %s", e)

    return {
        "changed": changed_pages,
        "errors": errors,
        "sections_total_changed": changed_sections_total,
        "details": details,
    }

def get_stats() -> dict:
    return get_cache_stats()
