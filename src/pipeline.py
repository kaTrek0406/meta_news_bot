# -*- coding: utf-8 -*-
import logging
import os
import json
import time
import random
from datetime import datetime, timezone
from typing import Dict, List, Tuple, Any, Optional

import asyncio
import httpx
from difflib import SequenceMatcher
import re

from .storage import load_cache, save_cache, compute_hash, get_cache_stats
from .config import (
    PROJECT_ROOT, SOURCES, USE_PROXY, PROXY_URL, PROXY_URL_EU,
    PROXY_PROVIDER, PROXY_STICKY, PROXY_FALLBACK_EU
)
from .html_clean import clean_html
from .summarize import summarize_rules, normalize_plain, extract_sections

log = logging.getLogger(__name__)

TRANS_CACHE_FILE = PROJECT_ROOT / "data" / "trans_cache.json"
if TRANS_CACHE_FILE.exists():
    try:
        trans_cache: Dict[str, str] = json.loads(TRANS_CACHE_FILE.read_text(encoding="utf-8"))
    except Exception:
        trans_cache = {}
else:
    trans_cache = {}

TIMEOUT = httpx.Timeout(30.0, connect=15.0)  # –£–≤–µ–ª–∏—á–∏–ª–∏ timeout

<<<<<<< Updated upstream
# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–∫—Å–∏ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
PROXY_HOST = os.getenv("PROXY_HOST", "")
PROXY_USER = os.getenv("PROXY_USER", "")
PROXY_PASSWORD = os.getenv("PROXY_PASSWORD", "")

def _get_proxy_config(session_id: Optional[str] = None) -> Optional[Dict[str, str]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–∫—Å–∏ –¥–ª—è httpx, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã.
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - session_id: –ò–î —Å–µ—Å—Å–∏–∏ –¥–ª—è BrightData (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–¥–∏–Ω IP –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤)
    """
    if PROXY_HOST and PROXY_USER and PROXY_PASSWORD:
        proxy_user = PROXY_USER
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–≤—è–∑–∫—É –∫ —Å—Ç—Ä–∞–Ω–µ –∏ session ID
        # –ü—Ä–∏–≤—è–∑—ã–≤–∞–µ–º –∫ —Å—Ç—Ä–∞–Ω–µ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ TARGET_REGION != AUTO
        if "-country-" not in proxy_user and TARGET_REGION != "AUTO":
            country_code = TARGET_REGION.lower()
            proxy_user = f"{proxy_user}-country-{country_code}"
        
        # –î–æ–±–∞–≤–ª—è–µ–º session ID –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–¥–Ω–æ–≥–æ IP
        if session_id and "-session-" not in proxy_user:
            proxy_user = f"{proxy_user}-session-{session_id}"
        
        proxy_url = f"http://{proxy_user}:{PROXY_PASSWORD}@{PROXY_HOST}"
        log.info(f"üîê –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–∫—Å–∏: {PROXY_HOST} (—Å—Ç—Ä–∞–Ω–∞: {TARGET_REGION}, session: {session_id or '–Ω–µ—Ç'})")
        return {"http://": proxy_url, "https://": proxy_url}
    else:
        log.warning("‚ö†Ô∏è –ü—Ä–æ–∫—Å–∏ –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω, –∑–∞–ø—Ä–æ—Å—ã –∏–¥—É—Ç –Ω–∞–ø—Ä—è–º—É—é")
        return None

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–µ–≥–∏–æ–Ω–∞ (–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è)
TARGET_REGION = os.getenv("TARGET_REGION", "MD")  # MD=Moldova (EU), US=United States

# –Ø–∑—ã–∫–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º
REGION_SETTINGS = {
    "MD": {
        "lang": "en-GB,en;q=0.9,ro;q=0.8,ru;q=0.7",  # English (UK/EU), Romanian, Russian
        "country": "MD",
        "timezone": "Europe/Chisinau"
    },
    "US": {
        "lang": "en-US,en;q=0.9",
        "country": "US",
        "timezone": "America/New_York"
    },
    "EU": {
        "lang": "en-GB,en;q=0.9",
        "country": "GB",  # UK as EU representative
        "timezone": "Europe/London"
    }
=======
# –Ø–∑—ã–∫–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º (–¥–ª—è Accept-Language)
_DEFAULT_LANG_BY_REGION = {
    "EU": "en-GB,en;q=0.9",
    "MD": "en-GB,en;q=0.9,ro;q=0.8,ru;q=0.7",
    "GLOBAL": "en-US,en;q=0.9",
>>>>>>> Stashed changes
}

def _get_proxy_for_region(region: str, proxy_country: Optional[str] = None, session_id: Optional[str] = None) -> Optional[Dict[str, str]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–∫—Å–∏ –¥–ª—è httpx –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–≥–∏–æ–Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞.
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - region: EU, MD –∏–ª–∏ GLOBAL
    - proxy_country: –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω—ã –¥–ª—è –ø—Ä–æ–∫—Å–∏ (–∏–∑ config.json –∏—Å—Ç–æ—á–Ω–∏–∫–∞)
    - session_id: –¥–ª—è sticky-—Å–µ—Å—Å–∏–π (Froxy –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç session=<rand>)
    """
    if not USE_PROXY:
        return None
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫–æ–π –ø—Ä–æ–∫—Å–∏ URL –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
    if region == "MD" and PROXY_URL:
        base_url = PROXY_URL
    elif region == "EU" and PROXY_URL_EU:
        base_url = PROXY_URL_EU
    elif PROXY_URL:
        base_url = PROXY_URL
    else:
        return None
    
    # –î–ª—è Froxy —Ñ–æ—Ä–º–∞—Ç: http://USER:PASS@proxy.froxy.com:9000
    # —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –≤ user: wifi;md;; –∏–ª–∏ session=<rand>
    if PROXY_PROVIDER == "froxy":
        # Froxy: –¥–æ–±–∞–≤–ª—è–µ–º session –≤ –ø–∞—Ä–æ–ª—å (—á–µ—Ä–µ–∑ wifi;md;;:)
        # –§–æ—Ä–º–∞—Ç wifi;md;; –æ–∑–Ω–∞—á–∞–µ—Ç: wifi (—Ç–∏–ø), md (—Å—Ç—Ä–∞–Ω–∞), –ø—É—Å—Ç—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if PROXY_STICKY and session_id:
            # –î–æ–±–∞–≤–ª—è–µ–º session –≤ –ø–∞—Ä–æ–ª—å
            # –î–ª—è Froxy –¥–æ–±–∞–≤–ª—è–µ–º session=<rand> –≤ –ø–∞—Ä–æ–ª—å
            modified_url = base_url.replace("@proxy.froxy.com", f":session={session_id}@proxy.froxy.com")
            log.debug(f"üîê Froxy sticky session: region={region}, session={session_id}")
            return {"http://": modified_url, "https://": modified_url}
        else:
            log.debug(f"üîê Froxy –ø—Ä–æ–∫—Å–∏: region={region}")
            return {"http://": base_url, "https://": base_url}
    else:
        # –î—Ä—É–≥–∏–µ –ø—Ä–æ–∫—Å–∏-–ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã
        log.debug(f"üîê –ü—Ä–æ–∫—Å–∏: region={region}, provider={PROXY_PROVIDER}")
        return {"http://": base_url, "https://": base_url}


# –†–æ—Ç–∞—Ü–∏—è User-Agent –¥–ª—è –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:132.0) Gecko/20100101 Firefox/132.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.6 Safari/605.1.15",
]

def _get_random_headers(url: str = "", accept_lang: Optional[str] = None):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
    ua = random.choice(USER_AGENTS)
    headers = {
        "User-Agent": ua,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": accept_lang or "en-US,en;q=0.9",
        "Cache-Control": "max-age=0",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Sec-Fetch-User": "?1",
        "Sec-CH-UA": '"Not_A Brand";v="8", "Chromium";v="131"',
        "Sec-CH-UA-Mobile": "?0",
        "Sec-CH-UA-Platform": '"Windows"',
    }
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è WhatsApp
    if "whatsapp.com" in url:
        headers["Referer"] = "https://www.google.com/"
        headers["Sec-Fetch-Site"] = "cross-site"
    
    return headers


FETCH_RETRIES = int(os.getenv("FETCH_RETRIES", "3"))
FETCH_RETRY_BACKOFF = float(os.getenv("FETCH_RETRY_BACKOFF", "1.2"))

LLM_MAX_CONCURRENCY = int(os.getenv("LLM_MAX_CONCURRENCY", "2"))
LLM_MIN_INTERVAL = float(os.getenv("LLM_MIN_INTERVAL", "0.3"))

# –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –æ—Ç —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –í–ö–õ)
PRUNE_REMOVED_SOURCES = os.getenv("PRUNE_REMOVED_SOURCES", "1") == "1"

_llm_sem = asyncio.Semaphore(LLM_MAX_CONCURRENCY)
_llm_lock = asyncio.Lock()
_last_llm_ts: float = 0.0

_SENT_SPLIT_RE = r"(?<=[\.\!\?\n])\s+"

def _split_sentences(text: str) -> List[str]:
    t = (text or "").strip()
    if not t:
        return []
    t = re.sub(r"\s+", " ", t)
    parts = re.split(_SENT_SPLIT_RE, t)
    out = []
    for p in parts:
        p = p.strip(" -‚Äì‚Äî‚Ä¢\u00a0\t")
        if len(p) >= 2:
            out.append(p)
    return out

def _pair_changed_sentences(old_sents: List[str], new_sents: List[str], threshold: float = 0.0):
    matched_pairs: List[Tuple[str, str]] = []
    old_set = set(old_sents)
    new_set = set(new_sents)
    same = old_set & new_set

    old_only = [s for s in old_sents if s not in same]
    new_only = [s for s in new_sents if s not in same]

    used_new_idx: set[int] = set()
    for s_old in old_only:
        best_j = -1
        best_score = 0.0
        for j, s_new in enumerate(new_only):
            if j in used_new_idx:
                continue
            score = SequenceMatcher(None, s_old, s_new).ratio()
            if score > best_score:
                best_score = score
                best_j = j
        if best_score > threshold and best_j >= 0:
            matched_pairs.append((s_old, new_only[best_j]))
            used_new_idx.add(best_j)

    paired_old = {w for w, _ in matched_pairs}
    paired_new = {n for _, n in matched_pairs}
    old_only_final = [s for s in old_only if s not in paired_old]
    new_only_final = [s for s in new_only if s not in paired_new]

    return matched_pairs, old_only_final, new_only_final

def _clip_line(s: str, limit: int = 800) -> str:
    s = (s or "").strip()
    if len(s) <= limit:
        return s
    return s[:limit-1].rstrip() + "‚Ä¶"


async def _summarize_async(plain: str) -> str:
    global _last_llm_ts
    async with _llm_sem:
        async with _llm_lock:
            now = time.monotonic()
            wait = LLM_MIN_INTERVAL - (now - _last_llm_ts)
            if wait > 0:
                await asyncio.sleep(wait)
            _last_llm_ts = time.monotonic()
        return await asyncio.to_thread(summarize_rules, plain)

async def run_update() -> dict:
    errors: List[Dict[str, Any]] = []
    details: List[Dict[str, Any]] = []

    cache_data = load_cache() or {}
    cache: List[Dict[str, Any]] = cache_data.get("items", [])
    # –ö–ª—é—á —Ç–µ–ø–µ—Ä—å (tag, url, region)
    idx: Dict[Tuple[str, str, str], int] = {
        (it.get("tag"), it.get("url"), it.get("region", "GLOBAL")): i 
        for i, it in enumerate(cache) if isinstance(it, dict)
    }

    changed_pages = 0
    changed_sections_total = 0

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º session ID –¥–ª—è sticky-—Å–µ—Å—Å–∏–π
    session_id = f"rand{random.randint(10000, 99999)}" if PROXY_STICKY else None
    
    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –±—É–¥–µ–º —Å–æ–∑–¥–∞–≤–∞—Ç—å —Å–≤–æ–π –∫–ª–∏–µ–Ω—Ç —Å –ø–æ–¥—Ö–æ–¥—è—â–∏–º –ø—Ä–æ–∫—Å–∏
    for src_idx, src in enumerate(SOURCES):
        tag, url, title_hint = src.get("tag"), src.get("url"), src.get("title")
        region = src.get("region", "GLOBAL")
        custom_lang = src.get("lang")  # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
        proxy_country = src.get("proxy_country")  # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
        
        if not tag or not url:
            continue
        
        # –ó–∞–¥–µ—Ä–∂–∫–∏ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        if src_idx > 0:
            if "whatsapp.com" in url:
                delay = 45.0 + random.random() * 15.0
                log.info(f"üí¨ ‚è≥ WhatsApp: –æ–∂–∏–¥–∞–Ω–∏–µ {delay:.1f} —Å–µ–∫ (—É–≤–µ–ª–∏—á–µ–Ω–Ω–∞—è –ø–∞—É–∑–∞)...")
            else:
                delay = (50.0 if random.random() < 0.3 else 60.0) + random.random() * 10.0
                log.info(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {delay:.1f} —Å–µ–∫ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –∑–∞–ø—Ä–æ—Å–æ–º...")
            await asyncio.sleep(delay)
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–∫—Å–∏ –¥–ª—è —Ä–µ–≥–∏–æ–Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        proxies = _get_proxy_for_region(region, proxy_country, session_id)
        
        # Accept-Language –ø–æ —Ä–µ–≥–∏–æ–Ω—É –∏–ª–∏ –∫–∞—Å—Ç–æ–º–Ω—ã–π
        accept_lang = custom_lang or _DEFAULT_LANG_BY_REGION.get(region, "en-US,en;q=0.9")
        headers = _get_random_headers(url, accept_lang)
        
        # SSL –ø—Ä–æ–≤–µ—Ä–∫–∞: –æ—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–∏ –ø—Ä–æ–∫—Å–∏
        verify_ssl = proxies is None
        
        html = None
        used_fallback = False
        
        async with httpx.AsyncClient(timeout=TIMEOUT, follow_redirects=True, proxies=proxies, verify=verify_ssl) as client:
            try:
                # Retry –ª–æ–≥–∏–∫–∞
                err = None
                for attempt in range(FETCH_RETRIES):
                    try:
                        r = await client.get(url, headers=headers)
                        r.raise_for_status()
                        html = r.text
                        
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
                        if "You're Temporarily Blocked" in html or "going too fast" in html:
                            raise httpx.HTTPStatusError("Temporary block", request=r.request, response=r)
                        
                        break  # –£—Å–ø–µ—à–Ω–æ!
                    except httpx.HTTPStatusError as e:
                        status = getattr(e.response, 'status_code', 0) if hasattr(e, 'response') else 0
                        
                        # 407/403 –¥–ª—è MD -> –ø—Ä–æ–±—É–µ–º fallback –Ω–∞ EU
                        if status in (407, 403) and region == "MD" and PROXY_FALLBACK_EU and PROXY_URL_EU and attempt == 0:
                            log.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ {status} –¥–ª—è MD –ø—Ä–æ–∫—Å–∏, –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ EU fallback...")
                            # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ EU –ø—Ä–æ–∫—Å–∏
                            proxies = _get_proxy_for_region("EU", proxy_country, session_id)
                            used_fallback = True
                            await asyncio.sleep(5)
                            continue
                        
                        if status in (502, 503, 429, 403, 407):
                            err = e
                            if attempt < FETCH_RETRIES - 1:
                                backoff = FETCH_RETRY_BACKOFF * (3 ** attempt) + random.random() * 5
                                log.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ {status} –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}, –ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/{FETCH_RETRIES}, –æ–∂–∏–¥–∞–Ω–∏–µ {backoff:.1f} —Å–µ–∫...")
                                await asyncio.sleep(backoff)
                                headers = _get_random_headers(url, accept_lang)
                            else:
                                if status == 429:
                                    log.error(f"‚ùå Facebook –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –∑–∞–ø—Ä–æ—Å—ã: {url}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
                                    err = None
                                    break
                                raise
                        else:
                            raise
                    except Exception as e:
                        err = e
                        if attempt < FETCH_RETRIES - 1:
                            backoff = FETCH_RETRY_BACKOFF * (2 ** attempt)
                            await asyncio.sleep(backoff)
                            headers = _get_random_headers(url, accept_lang)
                        else:
                            raise
                else:
                    if err:
                        raise err
            except Exception as e:
                log.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ %s: %s", url, e)
                errors.append({"tag": tag, "url": url, "region": region, "error": str(e)})
                continue
        
        if not html:
            errors.append({"tag": tag, "url": url, "region": region, "error": "No HTML received"})
            continue
        
        if used_fallback:
            log.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω–æ —á–µ—Ä–µ–∑ EU fallback: {url}")
        
        title_auto, full_plain, cleaned_html = clean_html(html, url)

            plain_norm = normalize_plain(full_plain or "")
            page_sig = compute_hash(plain_norm)

            sections_new = extract_sections(cleaned_html or html)
            sec_map_new = {s["id"]: s for s in sections_new if s.get("id")}

        key = (tag, url, region)
        existing_i = idx.get(key)
        existing = cache[existing_i] if existing_i is not None else None

            added_ids, removed_ids, modified_ids = [], [], []

            if existing:
                old_sections = existing.get("sections") or []
                sec_map_old = {s.get("id"): s for s in old_sections if s.get("id")}
                new_ids = set(sec_map_new.keys())
                old_ids = set(sec_map_old.keys())
                added_ids = list(new_ids - old_ids)
                removed_ids = list(old_ids - new_ids)
                modified_ids = [sid for sid in (new_ids & old_ids)
                                if sec_map_new[sid].get("sig") != sec_map_old[sid].get("sig")]
            else:
                added_ids = list(sec_map_new.keys())

            changed_here = bool(
                added_ids or removed_ids or modified_ids or
                (existing is None) or
                (existing and existing.get("hash") != page_sig)
            )
        if not changed_here:
            continue
        
        if page_sig in trans_cache:
            summary = trans_cache[page_sig]
        else:
            summary = await _summarize_async(full_plain or "")
            trans_cache[page_sig] = summary
        
        title = (title_hint or title_auto or "").strip() or url
        
        old_full = (existing or {}).get("full_text") or ""
        new_full = full_plain or ""
        old_sents = _split_sentences(old_full)
        new_sents = _split_sentences(new_full)
        pairs_global, old_only_global, new_only_global = _pair_changed_sentences(
            old_sents, new_sents, threshold=0.0
        )

            global_diff = {
                "changed": [{"was": _clip_line(w), "now": _clip_line(n)} for (w, n) in pairs_global],
                "removed": [_clip_line(s) for s in old_only_global],
                "added": [_clip_line(s) for s in new_only_global],
            }

            section_diffs: List[Dict[str, Any]] = []
            if added_ids:
                added_preview = []
                for sid in added_ids:
                    sents = _split_sentences(sec_map_new[sid].get("text") or "")
                    added_preview.append(_clip_line(sents[0] if sents else (sec_map_new[sid].get("title") or sid)))
                section_diffs.append({"type": "added", "title": "–î–æ–±–∞–≤–ª–µ–Ω–æ", "added": added_preview})

            if removed_ids:
                removed_titles = []
                for sid in removed_ids:
                    old_sec = next((s for s in (existing or {}).get("sections", []) if s.get("id") == sid), None)
                    ttl = (old_sec or {}).get("title") or sid
                    removed_titles.append(_clip_line(ttl))
                section_diffs.append({"type": "removed", "title": "–£–¥–∞–ª–µ–Ω–æ", "removed": removed_titles})

            if modified_ids:
                for sid in modified_ids:
                    old_s = next((s for s in (existing or {}).get("sections", []) if s.get("id") == sid), {})
                    new_s = sec_map_new[sid]
                    old_txt = old_s.get("text") or ""
                    new_txt = new_s.get("text") or ""
                    old_sents_s = _split_sentences(old_txt)
                    new_sents_s = _split_sentences(new_txt)
                    pairs_s, old_only_s, new_only_s = _pair_changed_sentences(
                        old_sents_s, new_sents_s, threshold=0.0
                    )
                    block = {
                        "type": "changed",
                        "title": new_s.get("title") or sid,
                        "changed": [{"was": _clip_line(w), "now": _clip_line(n)} for (w, n) in pairs_s]
                    }
                    if old_only_s:
                        block["removed_inline"] = [_clip_line(s) for s in old_only_s]
                    if new_only_s:
                        block["added_inline"] = [_clip_line(s) for s in new_only_s]
                    section_diffs.append(block)

        item = {
            "tag": tag,
            "url": url,
            "region": region,  # ‚ú® –¥–æ–±–∞–≤–ª–µ–Ω region
            "title": title,
            "summary": (summary or "").strip(),
            "ts": datetime.now(timezone.utc).isoformat(timespec="seconds"),
            "hash": page_sig,
            "sections": sections_new,
            "full_text": new_full,
            "last_changed_at": datetime.now(timezone.utc).isoformat(timespec="seconds"),
        }

        if existing_i is not None:
            cache[existing_i] = item
        else:
            cache.append(item)
            idx[key] = len(cache) - 1
        
        changed_pages += 1
        changed_sections_total += len(added_ids) + len(modified_ids) + len(removed_ids)
        
        details.append({
            "tag": tag,
            "url": url,
            "region": region,  # ‚ú® –¥–æ–±–∞–≤–ª–µ–Ω region
            "title": title,
            "diff": {
                "added": [sec_map_new[sid].get("title") or sid for sid in added_ids],
                "modified": [sec_map_new[sid].get("title") or sid for sid in modified_ids],
                "removed": [
                    (next((s.get("title") for s in (existing or {}).get("sections", [])
                           if s.get("id") == sid), sid))
                    for sid in removed_ids
                ],
            },
            "global_diff": global_diff,
            "section_diffs": section_diffs
        })

    # üîß –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —á–∏—Å—Ç–∏–º –∫—ç—à –æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –±–æ–ª—å—à–µ –Ω–µ—Ç –≤ config.json
    if PRUNE_REMOVED_SOURCES:
        # –ö–ª—é—á —Ç–µ–ø–µ—Ä—å (tag, url, region)
        valid_tuples = {(s.get("tag"), s.get("url"), s.get("region", "GLOBAL")) for s in SOURCES if s.get("tag") and s.get("url")}
        cache = [it for it in cache if (it.get("tag"), it.get("url"), it.get("region", "GLOBAL")) in valid_tuples]

    stats = get_cache_stats()
    cache.sort(key=lambda x: x.get("ts", ""), reverse=True)
    if stats.get("max_cache") and len(cache) > stats["max_cache"]:
        cache = cache[:stats["max_cache"]]

    cache_data["items"] = cache
    save_cache(cache_data)

    try:
        tmp = TRANS_CACHE_FILE.with_suffix(".tmp")
        with open(tmp, "w", encoding="utf-8") as tf:
            json.dump(trans_cache, tf, ensure_ascii=False, indent=2)
        os.replace(tmp, TRANS_CACHE_FILE)
    except Exception as e:
        log.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à –ø–µ—Ä–µ–≤–æ–¥–æ–≤: %s", e)

    return {
        "changed": changed_pages,
        "errors": errors,
        "sections_total_changed": changed_sections_total,
        "details": details,
    }

def get_stats() -> dict:
    return get_cache_stats()
